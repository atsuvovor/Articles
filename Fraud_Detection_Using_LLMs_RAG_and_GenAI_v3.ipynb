{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNS3zi6L94dHlf92PHAQJCK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atsuvovor/Pub_Data_Analytics_Project/blob/main/Fraud_Detection_Using_LLMs_RAG_and_GenAI_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Project: Fraud Detection Using Machine Learning, Retrieval-Augmented Generation (RAG), and Generative AI**"
      ],
      "metadata": {
        "id": "XWLmcwUPV6-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Toronto, January 17 2025**  \n",
        "**Autor : Atsu Vovor**\n",
        ">Master of Management in Artificial Intelligence    \n",
        ">Consultant Data Analytics Specialist | Machine Learning |  \n",
        "Data science | Quantitative Analysis |French & English Bilingual  \n"
      ],
      "metadata": {
        "id": "mdvfsfP3VhHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Abstract\n",
        "This project focuses on designing and developing generative AI solutions for finance and cybersecurity use cases, specifically fraud detection. By integrating machine learning models with retrieval-augmented generation (RAG) pipelines, the project enhances retrieval and generation tasks while optimizing large language model (LLM) performance through fine-tuning and strategic prompting. The system addresses challenges related to speed, performance, and cost-effectiveness, offering a scalable and efficient fraud detection framework. Detailed documentation of methodologies, results, and limitations ensures accessibility for diverse audiences.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Introduction\n",
        "Fraud detection is critical for mitigating financial losses and ensuring the integrity of digital transactions. The increasing sophistication of fraud tactics necessitates innovative solutions that combine machine learning, retrieval-augmented generation, and generative AI technologies. This project enhances fraud detection accuracy and interpretability by optimizing LLMs and RAG pipelines, addressing challenges such as latency, cost-effectiveness, and real-time decision-making.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Objectives\n",
        "1. **Design and develop generative AI solutions** for finance and cybersecurity use cases, focusing on fraud detection.\n",
        "2. **Implement RAG pipelines** to enhance retrieval and generation tasks by integrating knowledge bases with LLMs.\n",
        "3. **Optimize LLM performance** through fine-tuning, prompting strategies, and rigorous model evaluation.\n",
        "4. **Address challenges** related to speed, performance, and cost-effectiveness in deploying LLMs for real-time fraud detection.\n",
        "5. **Document methodologies, results, and limitations** for technical and non-technical audiences.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Project Description\n",
        "The fraud detection system includes:\n",
        "1. A **knowledge base** embedded and indexed for retrieval tasks.\n",
        "2. A machine learning model trained on transaction data for fraud prediction.\n",
        "3. RAG pipelines that integrate predictions with contextual knowledge explanations.\n",
        "4. Real-time optimization of LLMs to improve latency and cost-effectiveness.\n",
        "5. Comprehensive evaluation metrics, including precision, recall, F1-score, latency, and confusion matrix analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Project Scope\n",
        "This project aims to:\n",
        "1. Detect fraudulent financial transactions with high accuracy.\n",
        "2. Enhance interpretability through a retrieval-augmented generation pipeline.\n",
        "3. Optimize LLMs for real-time fraud detection tasks.\n",
        "4. Provide detailed documentation for reproducibility and diverse audience comprehension.\n",
        "5. Ensure extensibility for additional datasets and use cases.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Data Collection\n",
        "#### 5.1 Data Sources\n",
        "1. **Transaction Data**:\n",
        "   - Simulated or real transaction datasets containing attributes like transaction ID, user ID, amount, location, device type, and timestamp.\n",
        "   - Fraud labels (binary: 0 for legitimate, 1 for fraud).\n",
        "2. **Knowledge Base**:\n",
        "   - Fraud-related insights, such as high-risk transaction thresholds and suspicious patterns.\n",
        "\n",
        "#### 5.2 Data Characteristics\n",
        "1. High class imbalance (~5% fraud cases).\n",
        "2. Diverse transaction contexts (e.g., geographic locations, device types).\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Exploratory Data Analysis (EDA)\n",
        "#### 6.1 Steps\n",
        "1. **Descriptive Statistics**:\n",
        "   - Summarize distributions of transaction attributes.\n",
        "2. **Fraud Analysis**:\n",
        "   - Compare features across fraudulent and legitimate transactions.\n",
        "3. **Class Imbalance Analysis**:\n",
        "   - Evaluate the extent of fraud cases and propose mitigation strategies.\n",
        "4. **Visualization**:\n",
        "   - Generate heatmaps, histograms, and boxplots to uncover patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Feature Engineering\n",
        "#### 7.1 Existing Features\n",
        "1. Transaction amount\n",
        "2. User ID\n",
        "3. Location\n",
        "4. Device type\n",
        "5. Timestamp\n",
        "\n",
        "#### 7.2 Engineered Features\n",
        "1. **Temporal Features**:\n",
        "   - Hour of transaction, day of the week.\n",
        "2. **Risk Indicators**:\n",
        "   - High-risk locations and amounts.\n",
        "3. **Behavioral Features**:\n",
        "   - User transaction frequency and deviations from historical patterns.\n",
        "4. **Encoded Variables**:\n",
        "   - One-hot or label encoding for categorical features.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. Model Development\n",
        "#### 8.1 Machine Learning Models\n",
        "1. **Base Models**:\n",
        "   - Random Forest, Gradient Boosting.\n",
        "2. **Advanced Models**:\n",
        "   - XGBoost, LightGBM for improved accuracy.\n",
        "\n",
        "#### 8.2 Retrieval-Augmented Generation (RAG) Pipeline\n",
        "1. **Knowledge Embedding**:\n",
        "   - Use SentenceTransformer for semantic encoding of knowledge.\n",
        "2. **FAISS Index**:\n",
        "   - Store and retrieve knowledge embeddings for contextual explanations.\n",
        "3. **Integration**:\n",
        "   - Combine RAG outputs with model predictions for interpretability.\n",
        "\n",
        "#### 8.3 LLM Optimization\n",
        "1. Fine-tune models on domain-specific datasets.\n",
        "2. Develop and test prompting strategies for effective retrieval and generation.\n",
        "3. Evaluate model performance using latency, accuracy, and cost metrics.\n",
        "\n",
        "#### 8.4 Training and Testing\n",
        "1. Train-test split (80/20).\n",
        "2. Cross-validation for hyperparameter tuning.\n",
        "3. Address class imbalance with SMOTE or cost-sensitive learning.\n",
        "\n",
        "---\n",
        "\n",
        "### 9. Model Evaluation\n",
        "#### Metrics\n",
        "1. **Accuracy**\n",
        "2. **Precision**\n",
        "3. **Recall**\n",
        "4. **F1-score**\n",
        "5. **Latency (ms)**\n",
        "6. **Cost-effectiveness**\n",
        "7. **Confusion Matrix**\n",
        "\n",
        "#### Visualization\n",
        "1. Precision-recall curve.\n",
        "2. ROC-AUC curve.\n",
        "3. Confusion matrix heatmap.\n",
        "\n",
        "---\n",
        "\n",
        "### 10. Deployment Plan\n",
        "#### 10.1 Infrastructure\n",
        "1. Cloud-based deployment for scalability.\n",
        "2. Integration with real-time data streams for fraud detection.\n",
        "\n",
        "#### 10.2 Monitoring and Maintenance\n",
        "1. Periodic model retraining.\n",
        "2. Continuous monitoring of latency, accuracy, and cost metrics.\n",
        "\n",
        "---\n",
        "\n",
        "### 11. Timeline and Milestones\n",
        "1. **Week 1-2**: Data collection and knowledge base creation.\n",
        "2. **Week 3-4**: EDA and feature engineering.\n",
        "3. **Week 5-6**: Model training and RAG pipeline development.\n",
        "4. **Week 7**: LLM fine-tuning and optimization.\n",
        "5. **Week 8**: Model evaluation and deployment preparation.\n",
        "6. **Week 9-10**: Deployment and documentation.\n",
        "\n",
        "---\n",
        "\n",
        "### 12. Deliverables\n",
        "1. **Fraud detection model with detailed metrics**.\n",
        "2. **Final transaction table** with predictions, metrics, and explanations.\n",
        "3. **Deployment-ready fraud detection system**.\n",
        "\n",
        "---\n",
        "\n",
        "### 13. Project Limitations\n",
        "1. **Data Quality**:\n",
        "   - Performance may vary with incomplete or inconsistent transaction data.\n",
        "2. **Real-Time Constraints**:\n",
        "   - High transaction volumes may affect system latency and performance.\n",
        "3. **Domain Generalization**:\n",
        "   - Models trained on specific datasets may not generalize well to other domains without retraining.\n",
        "4. **Cost**:\n",
        "   - Fine-tuning and deploying LLMs can be resource-intensive.\n",
        "5. **Interpretability**:\n",
        "   - Complex models and RAG pipelines may require additional effort to explain predictions to non-technical stakeholders.\n",
        "\n",
        "------\n",
        "### 14. Conclusion\n",
        "This project presents an innovative approach to fraud detection by integrating machine learning, RAG pipelines, and generative AI. By optimizing LLM performance and addressing deployment challenges, the system delivers high accuracy and interpretability, contributing significantly to finance and cybersecurity domains.\n",
        "\n"
      ],
      "metadata": {
        "id": "zSexJ95QCyvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Python Impplementation**\n",
        "\n",
        "### Key Features of the Script:\n",
        "1. **Modular Functions**: Each component (data simulation, RAG, ML model training, evaluation) is encapsulated in functions or classes.\n",
        "2. **Machine Learning (ML)**: A `RandomForestClassifier` model is trained for fraud detection.\n",
        "3. **RAG Pipeline**: Knowledge is embedded and retrieved using `SentenceTransformer` and `FAISS`.\n",
        "4. **Metrics and Visualization**: Includes precision, recall, F1-score, and confusion matrix. Results are displayed in a dashboard.\n",
        "5. **Generative AI (LLMs)**: `SentenceTransformer` for knowledge embedding and contextual retrieval."
      ],
      "metadata": {
        "id": "DznxO3sBTIPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment update"
      ],
      "metadata": {
        "id": "sOBAM3LNVGYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers sentence-transformers faiss sklearn pandas numpy\n",
        "#!pip install transformers sentence-transformers\n",
        "!pip install faiss-cpu\n",
        "#!pip install faiss-gpu\n",
        "#!pip install farm-haystack"
      ],
      "metadata": {
        "id": "pXSk6OGW4IpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning, message=\"X does not have valid feature names\")\n",
        "\n",
        "# Generate simulated transaction data\n",
        "def generate_transaction_data():\n",
        "    np.random.seed(42)\n",
        "    data = {\n",
        "        \"transaction_id\": range(1, 1001),\n",
        "        \"user_id\": np.random.randint(1, 100, 1000),\n",
        "        \"transaction_amount\": np.random.uniform(10, 10000, 1000),\n",
        "        \"location\": np.random.choice([\"US\", \"UK\", \"CA\", \"IN\", \"AU\"], 1000),\n",
        "        \"device_type\": np.random.choice([\"Mobile\", \"Desktop\", \"Tablet\"], 1000),\n",
        "        \"is_fraud\": np.random.choice([0, 1], 1000, p=[0.95, 0.05]),\n",
        "        \"transaction_time\": pd.date_range(\"2025-01-01\", periods=1000, freq=\"min\").to_series().sample(1000).values\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    df[\"hour\"] = pd.to_datetime(df[\"transaction_time\"]).dt.hour\n",
        "    df[\"is_high_risk_location\"] = df[\"location\"].isin([\"IN\", \"AU\"]).astype(int)\n",
        "    df[\"device_type_encoded\"] = df[\"device_type\"].astype(\"category\").cat.codes\n",
        "    display(df)\n",
        "    return df\n",
        "\n",
        "#--------------Explanatory Data Analysis(EDA)---------------------------------------\n",
        "# Descriptive statistics\n",
        "def descriptive_statistics(df):\n",
        "    print(\"\\n--- Descriptive Statistics ---\")\n",
        "    display(df.describe(include='all'))\n",
        "    print(\"\\nUnique Values per Column:\")\n",
        "    display(df.nunique())\n",
        "\n",
        "\n",
        "def fraud_analysis(df):\n",
        "    print(\"\\n--- Fraud Analysis ---\")\n",
        "\n",
        "    # Validate if the required columns are present\n",
        "    required_columns = [\"is_fraud\", \"device_type\", \"transaction_amount\", \"location\", \"hour\"]\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "    if missing_columns:\n",
        "        print(f\"Missing columns for analysis: {missing_columns}\")\n",
        "        return\n",
        "\n",
        "    # Fraud statistics: Calculate mean for numerical columns grouped by fraud status\n",
        "    fraud_stats = df.groupby(\"is_fraud\").mean(numeric_only=True)\n",
        "    print(\"\\nFraud Statistics (Mean Values by Fraud Status):\")\n",
        "    display(fraud_stats)\n",
        "\n",
        "    # Plot: Fraud Statistics (Mean Values by Fraud Status)\n",
        "    fraud_stats.plot(kind='bar', figsize=(10, 6))\n",
        "    plt.title(\"Fraud Statistics (Mean Values by Fraud Status)\")\n",
        "    plt.ylabel(\"Mean Value\")\n",
        "    plt.show()\n",
        "\n",
        "    # Proportions of fraud by category (device type)\n",
        "    print(\"\\nFraud Proportions by Device Type:\")\n",
        "    fraud_device_proportions = df.groupby(\"is_fraud\")[\"device_type\"].value_counts(normalize=True)\n",
        "    display(fraud_device_proportions)\n",
        "\n",
        "    # Plot: Fraud Proportions by Device Type\n",
        "    fraud_device_proportions.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
        "    plt.title(\"Fraud Proportions by Device Type\")\n",
        "    plt.ylabel(\"Proportion\")\n",
        "    plt.show()\n",
        "\n",
        "    # Fraud by location\n",
        "    print(\"\\nFraud Proportions by Location:\")\n",
        "    fraud_location_proportions = df.groupby(\"is_fraud\")[\"location\"].value_counts(normalize=True)\n",
        "    display(fraud_location_proportions)\n",
        "\n",
        "    # Plot: Fraud Proportions by Location\n",
        "    fraud_location_proportions.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
        "    plt.title(\"Fraud Proportions by Location\")\n",
        "    plt.ylabel(\"Proportion\")\n",
        "    plt.show()\n",
        "\n",
        "    # Insights: High-risk locations vs. fraud occurrences\n",
        "    high_risk_stats = df.groupby(\"is_high_risk_location\")[\"is_fraud\"].mean()\n",
        "    print(\"\\nFraud Rate in High-Risk Locations:\")\n",
        "    display(high_risk_stats)\n",
        "\n",
        "    # Plot: Fraud Rate in High-Risk Locations\n",
        "    high_risk_stats.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
        "    plt.title(\"Fraud Rate in High-Risk Locations\")\n",
        "    plt.ylabel(\"Proportion\")\n",
        "    plt.show()\n",
        "\n",
        "   # Fraud Rate by Hour of Transaction\n",
        "    print(\"\\nFraud Rate by Hour of Transaction:\")\n",
        "    hour_fraud_rate = df.groupby(\"hour\")[\"is_fraud\"].mean()\n",
        "    print(hour_fraud_rate)\n",
        "\n",
        "    # Plot: Fraud Rate by Hour of Transaction\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(x=hour_fraud_rate.index, y=hour_fraud_rate.values)\n",
        "    plt.title(\"Fraud Rate by Hour of Transaction\")\n",
        "    plt.xlabel(\"Hour of Day\")\n",
        "    plt.ylabel(\"Fraud Rate\")\n",
        "    plt.show()\n",
        "\n",
        "# Class imbalance analysis\n",
        "def class_imbalance_analysis(df):\n",
        "    print(\"\\n--- Class Imbalance Analysis ---\")\n",
        "    fraud_count = df[\"is_fraud\"].value_counts()\n",
        "    print(fraud_count)\n",
        "    fraud_ratio = fraud_count[1] / fraud_count[0]\n",
        "    print(f\"Fraud-to-Legitimate Ratio: {fraud_ratio:.4f}\")\n",
        "    if fraud_ratio < 0.1:\n",
        "        print(\"Significant class imbalance detected. Consider using SMOTE or class weighting.\")\n",
        "\n",
        "    # Class distribution of 'is_fraud'\n",
        "    class_distribution = df[\"is_fraud\"].value_counts(normalize=True)\n",
        "    print(\"\\nClass Distribution of 'is_fraud':\")\n",
        "    print(class_distribution)\n",
        "\n",
        "    # Plot: Class Distribution of 'is_fraud'\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.barplot(x=class_distribution.index, y=class_distribution.values)\n",
        "    plt.title(\"Class Distribution of 'is_fraud'\")\n",
        "    plt.xlabel(\"Fraud Status\")\n",
        "    plt.ylabel(\"Proportion\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot: Histogram of Transaction Amounts by Fraud Status\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.histplot(data=df, x=\"transaction_amount\", hue=\"is_fraud\", bins=30, kde=True)\n",
        "    plt.title(\"Transaction Amount Distribution by Fraud Status\")\n",
        "    plt.xlabel(\"Transaction Amount\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot: Transaction Amounts Distribution for Fraud vs Non-Fraud\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.boxplot(data=df, x=\"is_fraud\", y=\"transaction_amount\")\n",
        "    plt.title(\"Transaction Amounts by Fraud Status\")\n",
        "    plt.xlabel(\"Fraud Status\")\n",
        "    plt.ylabel(\"Transaction Amount\")\n",
        "    plt.show()\n",
        "\n",
        "# Visualization\n",
        "def create_visualizations(df):\n",
        "    print(\"\\n--- Generating Visualizations ---\")\n",
        "\n",
        "    # Exclude non-numeric columns for the correlation heatmap\n",
        "    numeric_df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "    # Heatmap of correlations\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(numeric_df.corr(), annot=True, cmap=\"coolwarm\")\n",
        "    plt.title(\"Correlation Heatmap\")\n",
        "    plt.show()\n",
        "\n",
        "    # Histogram of transaction amounts\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.histplot(data=df, x=\"transaction_amount\", hue=\"is_fraud\", bins=30, kde=True)\n",
        "    plt.title(\"Transaction Amount Distribution\")\n",
        "    plt.xlabel(\"Transaction Amount\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "\n",
        "    # Boxplot of transaction amounts by fraud status\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.boxplot(data=df, x=\"is_fraud\", y=\"transaction_amount\")\n",
        "    plt.title(\"Transaction Amounts by Fraud Status\")\n",
        "    plt.xlabel(\"Is Fraud\")\n",
        "    plt.ylabel(\"Transaction Amount\")\n",
        "    plt.show()\n",
        "\n",
        "#-------------------------------------------------------------------------\n",
        "def setup_knowledge_base():\n",
        "    \"\"\"Setup the knowledge base and return embeddings and FAISS index.\"\"\"\n",
        "    knowledge = [\n",
        "        \"Transactions above $5000 are considered high-risk.\",\n",
        "        \"Transactions from unknown locations are flagged.\",\n",
        "        \"Multiple transactions in a short time period may indicate fraud.\",\n",
        "        \"Transaction patterns deviating from history are suspicious.\"\n",
        "    ]\n",
        "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    embeddings = model.encode(knowledge)\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(embeddings)\n",
        "    return knowledge, model, index\n",
        "\n",
        "def train_model(df, features, target):\n",
        "    \"\"\"Train a Random Forest classifier and return the trained model and split data.\"\"\"\n",
        "    X = df[features]\n",
        "    y = df[target]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    clf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "    clf.fit(X_train, y_train)\n",
        "    X_test = pd.DataFrame(X_test, columns=X_train.columns)\n",
        "    #X_train = pd.DataFrame(X_train, columns=X_train.columns)\n",
        "    y_test = pd.DataFrame(y_test, columns=[target])\n",
        "    y_train = pd.DataFrame(y_train, columns=[target])\n",
        "    print(\"\\nModel trained successfully!\")\n",
        "\n",
        "    return clf, X_train, X_test, y_train, y_test\n",
        "\n",
        "def evaluate_model(y_test, y_pred):\n",
        "    \"\"\"Calculate and print performance metrics.\"\"\"\n",
        "    precision = precision_score(y_test, y_pred, zero_division=1)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    return precision, recall, f1, cm\n",
        "\n",
        "def detect_fraud(transaction, clf, model, knowledge, faiss_index):\n",
        "    \"\"\"Detect fraud for a given transaction and return prediction with explanations.\"\"\"\n",
        "    query = f\"Transaction of ${transaction['transaction_amount']} in {transaction['location']} using {transaction['device_type']}.\"\n",
        "    query_embedding = model.encode([query])\n",
        "    _, indices = faiss_index.search(np.array(query_embedding), 2)\n",
        "    related_knowledge = [knowledge[i] for i in indices[0]]\n",
        "    feature_values = [[\n",
        "        transaction[\"transaction_amount\"],\n",
        "        transaction[\"user_id\"],\n",
        "        transaction[\"hour\"],\n",
        "        transaction[\"is_high_risk_location\"],\n",
        "        transaction[\"device_type_encoded\"]\n",
        "    ]]\n",
        "    start_time = time.time()\n",
        "    fraud_prediction = clf.predict(feature_values)\n",
        "    fraud_probability = clf.predict_proba(feature_values)[0][1]\n",
        "    latency = time.time() - start_time\n",
        "    explanation = f\"The transaction is flagged because: {', '.join(related_knowledge)}\"\n",
        "    return bool(fraud_prediction[0]), fraud_probability, explanation, latency\n",
        "\n",
        "def append_results_to_table(df, clf, model, knowledge, faiss_index):\n",
        "    \"\"\"Append fraud detection results and metrics to the transaction table.\"\"\"\n",
        "    results = []\n",
        "    for _, row in df.iterrows():\n",
        "        prediction, probability, explanation, latency = detect_fraud(row, clf, model, knowledge, faiss_index)\n",
        "        results.append({\n",
        "            \"is_fraud_predicted\": prediction,\n",
        "            \"fraud_probability\": probability,\n",
        "            \"explanation\": explanation,\n",
        "            \"latency_ms\": latency * 1000  # Convert to milliseconds\n",
        "        })\n",
        "    results_df = pd.DataFrame(results)\n",
        "    return pd.concat([df.reset_index(drop=True), results_df], axis=1)\n",
        "\n",
        "def calculate_cost_effectiveness(df):\n",
        "    \"\"\"Calculate the cost-effectiveness metric based on predictions.\"\"\"\n",
        "    cost_per_fraud = 100  # Arbitrary cost for a fraudulent transaction\n",
        "    cost_saved = cost_per_fraud * df[df[\"is_fraud_predicted\"] & df[\"is_fraud\"]].shape[0]\n",
        "    return cost_saved\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------------\n",
        "# Main script\n",
        "def main():\n",
        "    # Generate transaction data\n",
        "    transactions = generate_transaction_data()\n",
        "\n",
        "    # EDA steps\n",
        "    descriptive_statistics(transactions)\n",
        "    fraud_analysis(transactions)\n",
        "    class_imbalance_analysis(transactions)\n",
        "    create_visualizations(transactions)\n",
        "\n",
        "     # Setup knowledge base\n",
        "    knowledge, embed_model, faiss_index = setup_knowledge_base()\n",
        "\n",
        "    # Define features and target\n",
        "    features = [\"transaction_amount\", \"user_id\", \"hour\", \"is_high_risk_location\", \"device_type_encoded\"]\n",
        "    target = \"is_fraud\"\n",
        "\n",
        "    # Train the model\n",
        "    clf, X_train, X_test, y_train, y_test = train_model(transactions, features, target)\n",
        "    #print(\"\\nModel trained successfully!\")\n",
        "    # Evaluate the model\n",
        "    y_pred = pd.DataFrame(clf.predict(X_test), columns=[\"is_fraud_predicted\"])\n",
        "    #y_pred = clf.predict(X_test)\n",
        "    if y_pred[\"is_fraud_predicted\"].sum() == 0:\n",
        "        print(\"\\nWarning: No positive predictions made!\")\n",
        "\n",
        "    print(\"\\ndisplay(X_train)\")\n",
        "    display(X_train)\n",
        "    print(\"\\ndisplay(X_test)\")\n",
        "    display(X_test)\n",
        "    print(\"\\ndisplay(y_test)\")\n",
        "    display(y_test)\n",
        "    print(\"\\ndisplay(y_pred)\")\n",
        "    display(y_pred)\n",
        "\n",
        "    print(\"\\nModel evaluation results:\")\n",
        "    precision, recall, f1, confusion_mat = evaluate_model(y_test, y_pred)\n",
        "    print(f\"Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n",
        "    print(f\"Confusion Matrix:\\n{confusion_mat}\")\n",
        "    # Classification report\n",
        "    print(f\"\\nClassification report:\\n{classification_report(y_test, y_pred)}\")\n",
        "\n",
        "    # Append fraud detection results to table\n",
        "    transactions_with_results = append_results_to_table(transactions, clf, embed_model, knowledge, faiss_index)\n",
        "\n",
        "    # Add model metrics to every row in the table\n",
        "    transactions_with_results[\"precision\"] = precision\n",
        "    transactions_with_results[\"recall\"] = recall\n",
        "    transactions_with_results[\"f1_score\"] = f1\n",
        "    transactions_with_results[\"confusion_matrix\"] = [confusion_mat.tolist()] * len(transactions_with_results)\n",
        "\n",
        "    # Calculate cost-effectiveness\n",
        "    cost_saved = calculate_cost_effectiveness(transactions_with_results)\n",
        "    transactions_with_results[\"cost_effectiveness\"] = cost_saved\n",
        "\n",
        "    # Print the final table with all metrics\n",
        "    print(\"Final Transactions with Fraud Detection Results and Metrics:\")\n",
        "    display(transactions_with_results.head())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "VIz9DR3Xxa4h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}